# Data Scientist

## Education
- BTech, Computer Science Engineering with Artificial Intelligence specialization | Parul University, Gujarat (_2020 - 2024_)

## Technical Skills

**OPERATING SYSTEM**: Windows

**SOFTWARE/TOOLS**: Visual Studio Code, Power BI, Excel, Git

**DATABASE/SERVER**: MySQL, PostgreSQL

**PROGRAMMING LANGUAGES**: C, C++, Python, SQL, HTML, CSS

**FRAMEWORKS**: Django, Streamlit

**PYTHON LIBRARIES**: Pandas, Matplotlib, Plotly, Seaborn, Numpy, Scikit-Learn, Keras, TensorFlow, NLTK, Django, streamlit, BeautifulSoup

## Projects
### Streamlit Exploratory Data Analysis App
[Streamlit_EDA_app ðŸ“Š](https://chandu-2122-streamlit-eda-app-main-ehhr37.streamlit.app/)

A one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution using ydata-profiling and Streamlit allowing users to conduct comprehensive EDA effortlessly.

#### Working
This app analyzes the uploaded CSV files, providing in-depth insights into the dataset's characteristics through exploratory data analysis techniques.

#### Features
1. Upload CSV: Allows users to upload their CSV files for analysis.

2. Profiling Report: Generates an exploratory data analysis report using ydata_profiling.

3. Example Dataset: Offers the option to explore an example dataset in the absence of an uploaded CSV file.

#### Libraries Used
1. numpy: used for various numerical operations, especially in generating example datasets or handling numerical data within the analysis.
2. pandas: used to read and manipulate CSV files, organize data into DataFrames, and facilitates data exploration and presentation.
3. streamlit: used as the primary framework for building the EDA web application, allowing easy integration of data visualizations, user inputs, and data analysis tools.
4. ydata_profiling: Used to create comprehensive and interactive exploratory data analysis reports based on the uploaded or example datasets, providing detailed insights into the data's characteristics.
5. streamlit_pandas_profiling: Facilitates the direct embedding of Pandas Profiling reports generated by ydata_profiling into the Streamlit app, allowing users to visualize and interact with the analysis within the app interface.

#### App Snippet
![streamlit_eda](/assets/streamlit_eda.jpeg)

#### Result 
Based on the uploaded CSV file or the example dataset, the EDA report is generated with the help of ydata-profiling and Streamlit on this app.

#### Conclusion
Streamlit framework made easy to build web application for machine learning by simplifying the creation of interactive and data-driven apps.

### Exploratory Data Analysis (EDA) on Rural Telangana Illiteracy Rates
[Github Repository](https://github.com/Chandu-2122/Power_BI)

#### Objective
#### Data Source
#### Dataset Description
#### Data Preperation
#### Visualizations
#### Insights Gained
#### Future Scope

### Web Scraping and Data Extraction from Amazon.in for Electronic Gadgets
[Github Repository](https://github.com/Chandu-2122/Web_Scrapping)

#### Objective
Collecting comprehensive data on electronic gadgets commonly used by software employees or students, such as laptops, tablets, smartphones, smartwatches, headphones, earphones, and earbuds. The focus is on extracting this data during the Diwali season, especially emphasizing offer deals.

#### Data Source
The data is scrapped from 'amazon.in' website.

#### Dataset Description
Our required data from the webpage:

**name**: Title of the product

**brand**: Brand of the product

**model name**: Model name of the product

**screen size**: Display size of the screen

**colour**: Colour of the product

**cpu model**: CPU model of the product

**ram memory installed size**: Installed size of ram memory in the product

**operating system**: Operating System of the product

**mrp**: Actual price of the product

**offer**: Offer on the product

**number of purchase in last month**: Number of purchases of the product in last month

**number of ratings**: Number of ratings received for the product

**rating**: Overall rating of the product

#### Project Structure
1. fuctions.py:

This file has all the functions required to extract the necessary data from the amazon.in website

2. main.py:

This file has the main part:

Web scrapping the required data from 20 pages in the URLs of required products(laptop, tab, smartphone, smartwatch, earphones, earbuds, headphones) and saving it as csv files.

#### Data Preperation
With the functions, required data was extracted from the webpage by finding the mentioned tags and if no such tag was found that value is replaced with an empty string.

And then, products having no title value were removed from the dataset and then saved as a csv file.

#### Libraries Used
1. **requests** : Used for sending HTTP requests to websites to fetch the HTML content of web pages

2. **BeautifulSoup (from bs4)** : Used for parsing the HTML content obtained using the 'requests' library

3. **pandas** : Used to create the DataFrame to organize and structure the scraped data

4. **numpy** : Used to replace null values

#### Results
Was successfully able to web scrape the amazon.in data once before the header i used got blocked or restricted.

#### Data Snippet

#### Conclusion 

Got to know that the success of web scrapping depends on various factors:

Headers: Many websites require a valid user-agent string which allows/blocks/restricts the reuests.

Website Policies: Some websites like amazon are having strict anti-scraping policies that are preventing or limiting scraping attempts.

Website Changes: As we are relying on specific HTML tags to web scrape the data, sometimes our code might not be able to find the required HTML elements if the website changes its structure.

